geom_bar(data=diamonds, aes(cut,price))
#qplot
qplot(x = cty, y = hwy, color = cyl, data = mpg, geom = "point")
#ggplot2
#ggplot(data = mpg, aes(x = cty, y = hwy))     #aes() 변수를 구성요소에 매핑. aesthetic 약자
ggplot(data= mpg, aes(x = cty, y = hwy)) +    #mpg 데이터사용, x축에 cty변수, y축에 hwy변수를 매핑
geom_point(aes(color=cyl)) +      #산점도 그리기. cyl별로 색깔을 지정
geom_smooth(method ="lm") +       #직선 그리기. 회귀직선 linear model
coord_cartesian() +               #데카르트 직교좌표게 사용. xlim= , ylim= 사용하여 축범위 설정
scale_color_gradient()+           #척도의 색깔을 점점다르게
theme_bw()                        #테마 사용
mpg %>% ggplot(aes(cty, hwy)) +     #위와 동일. mpg 데이터를 dplyr 라이브러리를 사용하여 표기
geom_point(aes(color=cyl)) +
geom_smooth(method ="lm") +
coord_cartesian() +
scale_color_gradient()+
theme_bw()
dataset %>% ggplot(aes(x축, y축, 옵션)) +
geom_smooth() : smooth line 과 se 를 표시 +
geom_boxplot(): 상자그림 +
geom_histogram() : 연속변수의 분포 +
geom_freqpoly() :  연속변수의 분포, 도수를 직선으로 연결 +
geom_density() : 확률분포밀도함수를 매끄러운 곡선으로 연결 +
geom_bar() : 막대그림  +
geom_path(): 점들을 자료 순서대로 선으로 연결한 그림 +
geom_line(): 산점도에서의 위치에 따라 왼쪽에서 오른쪽순으로 모든 점들을 연결
#아래 3가지 코드는 모두 동일한 그래프
ggplot(data= mpg, aes(x=displ, y=hwy)) +
geom_point()                      #geom_* 옵션을 주어  좌표 위에 표현이 가능
ggplot(mpg, aes(displ, hwy)) +
geom_point()
mpg %>% ggplot(aes(displ, hwy))+    #dplyr 라이브러리와 함께 데이터 전처리, 시각화하기
geom_point()                      # 오늘 세션에서는 편의상 3번째 표현으로 통일하겠음
# gapminder 데이터에서 1인당국내총생산(gdpPercap)의 도수 확인
a1<- gapminder %>% ggplot(aes(x=gdpPercap)) + geom_histogram()
a2<- gapminder %>% ggplot(aes(x=gdpPercap)) + geom_histogram()+
scale_x_log10()
a3<- gapminder %>% ggplot(aes(x=gdpPercap)) + geom_freqpoly()+
scale_x_log10()
a4<- gapminder %>% ggplot(aes(x=gdpPercap)) + geom_density()+
scale_x_log10()
grid.arrange(a1,a2,a3,a4)
diamonds %>% ggplot(aes(x=cut)) + geom_histogram()
diamonds %>% ggplot(aes(x=cut)) + geom_bar()
#diamonds 데이터에서 컷의품질(cut)별로 다이아몬드색깔(color) 정보 추가
b2<- diamonds %>% ggplot(aes(cut, fill=color))
b2+ geom_bar(position = "dodge")
b2+ geom_bar(position = "fill")
b2+ geom_bar(position = "stack")
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1, stat = "identity")
diamonds %>% ggplot(aes(x=cut)) + geom_bar()
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1)
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1) + coord_polar("y",start=0)
mpg %>% ggplot(aes(cty, hwy)) +     #위와 동일. mpg 데이터를 dplyr 라이브러리를 사용하여 표기
geom_point(aes(color=cyl)) +
geom_smooth(method ="lm") +
coord_cartesian() +
scale_color_gradient()+
theme_bw()
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1) + scale_fill_brewer(palett="Blues") + coord_polar("y",start=0)
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1) + scale_fill_brewer(palett="Blues")
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1) + scale_fill_brewer(palette="Blues")
diamonds %>% ggplot(aes(x=cut)) + geom_bar(width = 1) + scale_fill_brewer(palette="Blues")
diamonds %>% ggplot(aes(fill=cut)) + geom_bar() + scale_fill_brewer(palette="Blues")
diamonds %>% ggploy(aes(cut))
diamonds %>% ggplot(aes(cut))
diamonds %>% ggplot(aes(cut))
diamonds %>% ggplot(aes(cut)) + gem_bar
diamonds %>% ggplot(aes(cut)) + geom_bar
diamonds %>% ggplot(aes(cut)) + geom_bar()
diamonds %>% ggplot(aes(fill=cut)) %>% geom_bar() %>% scale_fill_brewer(palette="Blues")
diamonds %>% ggplot(aes(fill=cut)) %>% geom_bar() + scale_fill_brewer(palette="Blues")
diamonds %>% ggplot(aes(fill=cut)) + geom_bar() + scale_fill_brewer(palette="Blues")
diamonds + ggplot(aes(fill=cut)) + geom_bar() + scale_fill_brewer(palette="Blues")
#QUESTION 2.
# diamonds 데이터에서 컷의품질별(cut)로 bar chart의 막대색깔을 다르게 해보자
n<- b1 + geom_bar(aes(fill=cut));n
# diamonds 데이터에서 컷의품질별(cut)로 도수 확인
b1<- diamonds %>% ggplot(aes(cut))               #도수분포 막대그래프
b1 + geom_bar()
#QUESTION 2.
# diamonds 데이터에서 컷의품질별(cut)로 bar chart의 막대색깔을 다르게 해보자
n<- b1 + geom_bar(aes(fill=cut));n
n+ scale_fill_brewer(palette="Blues")
diamonds %>% ggplot(aes(cut)) + geom_bar(aes(fill=cut)) + scale_fill_brewer(palette="Blues")
coord_polar("y",start=0)
diamonds %>% ggplot(aes(cut)) + geom_bar(aes(fill=cut)) + scale_fill_brewer(palette="Blues") +coord_polar("y",start=0)
n+ scale_fill_grey(start=0.2, end=0.8, na.value = "red")
n+ scale_fill_brewer(palette="Blues")
n+ scale_fill_grey(start=0.2, end=0.8, na.value = "red")
diamonds %>% ggplot(aes(x=carat,y=color))
diamonds %>% ggplot(aes(x=carat,y=color)) + geom_bar()
head(diamonds)
diamonds %>% ggplot(aes(x=color,y=carat)) + geom_bar()
diamonds %>% ggplot(aes(color,carat)) + geom_bar()
diamonds %>% ggplot(aes(x=color, fill = carat)) + geom_bar()
diamonds %>% ggplot(aes(x=color, fill = carat)) + geom_histogram()
diamonds %>% ggplot(aes(x=color, fill = carat)) + geom_histogram()
gapminder %>% ggplot(aes(x=gdpPercap, fill= continent )) + geom_histogram()+
scale_x_log10() +
facet_wrap(~ continent )
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram()
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_bar()
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram()
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram(bins=30)
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram()
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram(binwidth = 30)
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram(binwidth = 1)
diamonds %>% ggplot(aes(x=carat, fill = color)) + geom_histogram(binwidth = 0.5)
diamonds %>% ggplot(aes(x=carat,fill=price)) + geom_point()
diamonds %>% ggplot(aes(carat,price)) + geom_point()
diamonds %>% filter(carat>3) %>% ggplot(aes(carat,price)) + geom_point()
diamonds %>% filter(carat>3) %>% ggplot(aes(carat,price),color=clarity,size=cut) + geom_point()
diamonds %>% filter(carat>=3) %>% ggplot(aes(carat,price),color=clarity,size=cut) + geom_point()
#5. 3ĳ?? ?̻??? ???̾Ƹ??????? carat?? price?? ???? scatterplot�� ?׸??ÿ?.
#color?? clarity?? ?ϰ?, point?? size?? cut��???Ͻÿ?
a5<-ggplot(diamonds%>%filter(carat>=3),aes(x=carat,y=price,color = clarity,size=cut))+geom_point();a5
diamonds %>% filter(carat>=3) %>% ggplot(aes(carat,price,color=clarity,size=cut)) + geom_point()
diamonds %>% filter(carat>=3) %>% ggplot(aes(carat,price,color=clarity,size=cut)) + geom_point()
sms_results <- read.csv("./sms_results.csv")
sms_results <- read.csv("C:\\Users\\test\\Desktop\\BOAZ\\assignment\\5th_evaluation\\sms_results.csv")
head(sms_results)
table(sms_results$actual_type, sms_results$predict_type)
kappa(sms_results)
######1.
# caret의 confusionMatrix 이용하여 predict된 결과의 성능을 확인하세요
## 기회가 된다면 Kappa 통계량에 대해 좀더 알아보세요
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(psych)) install.packages("psych"); library(psych)
library(psych)
cohen.kappa(sms_results)
a<-table(sms_results$actual_type, sms_results$predict_type)
cohen.kappa(a)
######2.
#glm 모델의 성능을 평가합니다
##ifelse 함수를 이용해 glm$fitted.values가 0.5 이상이면 yes, 아니면 no로 지정하고 결과를 out에 할당하세요(factor)
###역시 confusionMatrix로 성능을 확인하세요
credit <- read.csv("C:\\Users\\test\\Desktop\\BOAZ\\assignment\\5th_evaluation\\credit.csv")
glm <- glm(default ~ ., data=credit,family = binomial)
summary(glm)
head(glm$fitted.values)
glm$fitted.values[1]
out <- factor()
else out[i] == "no"
for i in len(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] == "yes"
}
else out[i] == "no"
}
for i in len(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] == "yes"
}
else {out[i] == "no"
}
}
out <- factor()
for i in len(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] == "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] == "no"
}
}
len(glm$fitted.values)
length(glm$fitted.values)
out <- factor()
for i in length(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] == "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] == "no"
}
}
glm$fitted.values[1]
glm$fitted.values[2]
glm$fitted.values[3]
glm$fitted.values[3] > 0.5
for i in length(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] <- "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] <- "no"
}
}
out[1] <- yes
out[1] <- 'yes'
table(credit$default,out)
out <- c()
for i in length(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] <- "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] <- "no"
}
}
out[1] <- "yes"
out
out[2] <- "no"
out
for i in 1:length(glm$fitted.values) {
if (glm$fitted.values[i] >= 0.5) {
out[i] <- "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] <- "no"
}
}
for (i in 1:length(glm$fitted.values)) {
if (glm$fitted.values[i] >= 0.5) {
out[i] <- "yes"
}
ifelse (glm$fitted.values[i] < 0.5) {
out[i] <- "no"
}
}
1:length(glm$fitted.values)
for (i in 1:length(glm$fitted.values)) {
if (glm$fitted.values[i] >= 0.5)
{
out[i] <- "yes"
}
else if (glm$fitted.values[i] < 0.5) {
out[i] <- "no"
}
}
table(credit$default,out)
set.seed(180808)
confusionMatrix(sms_results)
?confusionMatrix
cohen.kappa(a)
confusionMatrix(sms_results$predict_type,sms_results$actual_type)
?ifelse
#############ifelse version#############
out <- ifelse(glm$fitted.values >= 0.5, 'yes','no')
out
out <- as.factor(out)
print(out)
table(credit$default,out)
if(!require(ROCR)) install.packages("ROCR"); library(ROCR)
predict(sms_results$prob_spam,sms_results$actual_type)
sms_pred<-prediction(sms_results$prob_spam,sms_results$actual_type)
sms.perf <-performance(sms_pred, "tpr", "fpr") # ROC-chart
plot(sms.perf)
performance(sms.pred, "auc")@y.values[[1]]
performance(sms_pred, "auc")@y.values[[1]]
head(sms_results)
head(credit)
col(credit)
col.names(credit)
col_names(credit)
colnames(credit)
head(glm$fitted.values)
summary(glm)
table(credit$default,out)
credit_pred<-prediction(out,credit$default)
summary(glm)
head(credit$default)
head(sms_results)
?cbind
credit2 <- cbind(credit,out)
head(credit2)
credit_pred<-prediction(credit2$out,credit2$default)
sms_results$prob_spam
credit_pred<-prediction(glm$fitted.values,credit$default)
credit.perf <-performance(credit_pred, "tpr", "fpr") # ROC-chart
plot(credit.perf)
performance(credit_pred, "auc")@y.values[[1]]
?glm
summary(glm)
glm$coefficients
a<- glm$coefficients < 0.001
a
a<-c()
for (i in 1:length(glm$coefficients)) {
if (glm$coefficients[i] < 0.01) {}
a[i] <- glm$coefficients[i]
}
a
summary(glm)
glm
#############remove unimportant features################
glm <- glm(default ~ checking_balanceunknown + months_loan_duration + credit_historygood + credit_historyperfect + credit_historyvery good + savings_balanceunknown + employment_duration4 - 7 years + percent_of_income, data=credit,family = binomial)
?glm
head(credit)
#############remove unimportant features################
glm <- glm(default ~ checking_balance + credit_history + savings_balance + employment_duration, data=credit,family = binomial)
glm <- glm(default ~ ., data=credit,family = binomial)
summary(glm2)
#############remove unimportant features################
glm2 <- glm(default ~ checking_balance + credit_history + savings_balance + employment_duration, data=credit,family = binomial)
summary(glm2)
out2 <- ifelse(glm2$fitted.values >= 0.5, 'yes','no')
out <- as.factor(out2)
print(out)
table(credit$default,out2)
credit_pred2<-prediction(glm2$fitted.values,credit$default)
credit.perf2 <-performance(credit_pred2, "tpr", "fpr") # ROC-chart
plot(credit.perf2)
performance(credit_pred2, "auc")@y.values[[1]]
summary(glm2)
summary(glm)
#############remove unimportant features################
glm2 <- glm(default ~ checking_balance + credit_history + savings_balance + employment_duration + percent_of_income, data=credit,family = binomial)
summary(glm2)
out2 <- ifelse(glm2$fitted.values >= 0.5, 'yes','no')
out2 <- as.factor(out2)
table(credit$default,out2)
credit_pred2<-prediction(glm2$fitted.values,credit$default)
credit.perf2 <-performance(credit_pred2, "tpr", "fpr") # ROC-chart
plot(credit.perf2)
performance(credit_pred2, "auc")@y.values[[1]]
performance(credit_pred, "auc")@y.values[[1]]
dataset <- read.csv("./Social_Network_Ads.csv")
dataset <- read.csv("C://User/test/Desktop/BOAZ/study/glm발제/Social_Network_Ads.csv")
dataset <- read.csv(file = "Social_Network_Ads.csv")
head(dataset)
dataset = dataset[,3:5]
install.packages("caTools")
library(caTools)
split = sample.split(dataset$Purchased,SplitRatio=0.75)
training_set = subset(dataset,split==TRUE)
test_set = subset(dataset,split ==FALSE)
training_set[,1:2]  = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
head(test_set)
head(dataset)
##Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~.,
family = binomial,
data= training_set)
## Predicting the Test set results
prob_pred = predict(classifier, type = 'response',newdata=test_set[-3])
prob_pred
y_pred = ifelse(prob_pred >0.5,1,0)
y_pred
##Making the Confusion Matrix
cm = table(test_set[,3],y_pred)
cm
# Visualising the Training set results
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
head(set)
library(flexclust)
library(NbClust)
library(tidyverse)
fit_average=hclust(d,method='average')
plot(fit_average,hang=1,cex=.8,main='평균 연결법')
#내장 데이터셋 불러오기
data(nutrient,package='flexclust')
#확인
head(nutrient)
# 관측치간의 거리 구하기
d=as.matrix(dist(nutrient))
head(d)[1:4,1:4]
nutrient_scaled=scale(nutrient)
#스케일링 된 거리척도를 통해 각 관측치간에 거리 구하기
d=dist(nutrient_scaled)
fit_average=hclust(d,method='average')
plot(fit_average,hang=1,cex=.8,main='평균 연결법')
fit_complete=hclust(d,method='complete')
plot(fit_complete,hang=1,cex=.8,main='최장 연결법')
fit_single=hclust(d,method='single')
plot(fit_single,hang=1,cex=.8,main='최단 연결법')
# 스케일링된 데이터를 토대로 거리척도와 최대 군집, 최소군집수를 주면 알아서 구해줌
nc=NbClust(nutrient_scaled,distance = 'euclidean',
min.nc=2,max.nc=15,method='average')
par(mfrow=c(1,1))
# 지표에 따르면  2, 3, 5, 15가 최적의 군집수라는 결론에 이름
t1=table(nc$Best.n[1,]);t1
barplot(t1,xlab='Number of Clusters',ylab='Number of Criteria')
clusters_1=cutree(fit_average,k=5)
#원 데이터 프레임과 결합
nutrient_1=cbind(nutrient,clusters_1)
table(clusters_1)
ggplot(nutrient_1,aes(x=nutrient_1$energy,y=nutrient_1$protein,col=factor(nutrient_1$clusters_1)))+geom_point(size=3)
#클러스터 간의 피쳐별 중앙값
aggregate(nutrient,by=list(cluster=clusters_1),median)
par(mfrow=c(1,1))
plot(fit_average,hang=-1,cex=.8,main='Average Lingkage')
rect.hclust(fit_average,k=4)
wssplot=function(data,nc=15,seed=1234){
wss=(nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i]=sum(kmeans(data,centers=i)$withinss)
}
plot(1:nc,wss,type='b',xlab='number of clusters',
ylab='within groups sum of squares')
}
# 그래프 보면 군집이 5개 이상으로 가면 군집간의 분산이 그렇게 낮아지지 않음을 알수 있음.
wssplot(nutrient)
nc2=NbClust(nutrient,min.nc=2,max.nc=15,method='kmeans')
fit_km=kmeans(nutrient,4)
aggregate(nutrient,by=list(fit_km$cluster),mean)
#그래프로 시각화
ggplot(nutrient,aes(x=nutrient$energy,y=nutrient$protein,col=factor(fit_km$cluster)))+geom_point(size=3)
clusters_1=cutree(fit_average,k=5)
#원 데이터 프레임과 결합
nutrient_1=cbind(nutrient,clusters_1)
table(clusters_1)
ggplot(nutrient_1,aes(x=nutrient_1$energy,y=nutrient_1$protein,col=factor(nutrient_1$clusters_1)))+geom_point(size=3)
#클러스터 간의 피쳐별 중앙값
aggregate(nutrient,by=list(cluster=clusters_1),median)
par(mfrow=c(1,1))
plot(fit_average,hang=-1,cex=.8,main='Average Lingkage')
rect.hclust(fit_average,k=5)
rect.hclust(fit_average,k=4)
rect.hclust(fit_average,k=6)
packages = c("Rfacebook", "tm", "lsa", "wordcloud","ggplot2","KoNLP",
"GPArotation","cluster","RWeka","ROAuth","fpc","stringr","ape","devtools")
for (i in packages){
if(!require( i , character.only = TRUE))
{install.packages(i, dependencies = TRUE)}
}
#devtools::install_github('haven-jeon/KoSpacing')
library(KoSpacing) #if you interested in this package, visit here (https://github.com/haven-jeon/KoSpacing)
spacing("제발이것좀띄어쓰기해주세요왜이렇게띄어쓰기를제대로안하는사람이많아요보아즈여러분들은아닐거라고믿어요글쓸때는띄어쓰기꼭합시다")
setwd("D://Facebook_bamboo_project/")
useNIADic()
Posts <- read.csv("Bamboo_posts_17v.csv",header = TRUE)
Posts <- data.frame(Posts$Sepped.v2)
Posts <- as.data.frame(Posts[1:1000,])
posts_v1 <- as.character(Posts$`Posts[1:1000, ]`)
posts_v1 <- gsub("[[:punct:]]","",posts_v1)
posts_v1 <- gsub("\\s+"," ", posts_v1)
posts_v1 <- spacing(posts_v1) #more than 200 charaters, it doesn't working
ExtractWord <- function(doc){
doc <- as.character(doc)
doc2 <- paste(SimplePos22(doc))
doc3 <- str_match(doc2, "([가-힣]+)/NC")
if(dim(doc3)[2] == 2){
doc4 <- doc3[,2]
doc4 <- doc4[!is.na(doc4)]
return(doc4)
}
}
nouns = sapply(posts_v1, words, USE.NAMES = F)
txt_noun1 <- nouns
words <- read.csv("2nd_stopwords.txt",header=FALSE)
words <- as.character(words$V1)
corpus <- Corpus(VectorSource(nouns)) # assign vector value
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, words)
uniTokenizer <- function(x) unlist(strsplit(as.character(x), "[[:space:]]+"))
control = list(tokenize = uniTokenizer,
wordLengths=c(2,20),
stopwords = c("\\n","\n","것","c"),
weighting = function (x) {weightTfIdf(x, TRUE)})
dtm <- DocumentTermMatrix(corpus, control=control) #invert to dtm using tokenizer
object.size(dtm)
Encoding(dtm$dimnames$Terms) = "UTF-8" #fucking encoding...ha...
findFreqTerms(dtm,lowfreq = 5)
load(file="image1029.RData")
View(corpus)
View(txt_noun)
rm(control)
rm(cord)
rm(corpus)
rm(ex)
rm(fb_data)
rm(fb_data2)
rm(fbAuth)
rm(file)
rm(gframe)
rm(LSA)
rm(rot)
rm(st)
rm(td)
rm(txt_noun)
rm(txt2)
rm(text_lsa)
rm(wd)
rm(check)
rm(colTotal)
rm(i)
rm(myStopwords)
rm(packages)
rm(pal)
rm(signs)
rm(strength)
rm(t)
rm(TermFreq)
rm(TermFreq2)
rm(terms)
rm(tt)
rm(txt)
rm(txt1)
rm(scanner)
rm(uniTokenizer())
rm(uniTokenizer
)
save(tdm,file='C://Users/test/Desktop/tdm.RData')
